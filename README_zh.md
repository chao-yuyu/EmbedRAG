# EmbedRAG - Embedding Model Training Framework

ä¸€å€‹åŸºæ–¼Transformerçš„General Text Embeddingæ¨¡å‹è¨“ç·´æ¡†æ¶ï¼Œæ”¯æŒLoRAã€QLoRAå’Œå…¨åƒæ•¸å¾®èª¿ä¸‰ç¨®è¨“ç·´æ–¹å¼ï¼Œå°ˆç‚ºæª¢ç´¢å¢å¼·ç”Ÿæˆ(RAG)å ´æ™¯è¨­è¨ˆã€‚

## åŠŸèƒ½ç‰¹è‰²

- ğŸš€ æ”¯æŒå¤šç¨®å¾®èª¿ç­–ç•¥ï¼šLoRAã€QLoRA(4-bit/8-bit)ã€å…¨åƒæ•¸å¾®èª¿
- ğŸ“Š è©³ç´°çš„è¨“ç·´æ—¥èªŒå’Œç›£æ§
- ğŸ”§ éˆæ´»çš„åƒæ•¸é…ç½®
- ğŸ’¾ è‡ªå‹•æ¨¡å‹ä¿å­˜å’Œæª¢æŸ¥é»ç®¡ç†
- ğŸ”„ LoRA adapterèˆ‡base modelçš„ç„¡ç¸«åˆä½µ

## ç³»çµ±éœ€æ±‚

- Python 3.10+
- CUDA GPU
- PyTorch
- Transformers
- PEFT
- Accelerate

## å®‰è£ä¾è³´

```bash
pip install torch transformers peft accelerate datasets safetensors bitsandbytes
```

## è¨“ç·´è³‡æ–™æ ¼å¼

è¨“ç·´è³‡æ–™ä½¿ç”¨JSONLæ ¼å¼ï¼Œæ¯è¡ŒåŒ…å«ä¸€å€‹JSONå°è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
  "query": "æŸ¥è©¢æ–‡æœ¬",
  "pos": ["æ­£ä¾‹æ–‡æœ¬1", "æ­£ä¾‹æ–‡æœ¬2"],
  "neg": ["è² ä¾‹æ–‡æœ¬1", "è² ä¾‹æ–‡æœ¬2", "è² ä¾‹æ–‡æœ¬3"]
}
```

### ç¯„ä¾‹è³‡æ–™

```json
{"query": "å°åŒ—æœ€å¥½çš„ç‰›è‚‰éºµåº—", "pos": ["è€å¼µç‰›è‚‰éºµé¤¨ä½æ–¼å°åŒ—å¸‚ä¸­å¿ƒï¼Œä»¥å…¶æ¿ƒéƒæ¹¯é ­å’Œå«©æ»‘ç‰›è‚‰èåï¼Œæ˜¯ç•¶åœ°äººæ¨è–¦çš„å¿…åƒç¾é£Ÿ"], "neg": ["å°åŒ—101è§€æ™¯å°æä¾›360åº¦åŸå¸‚å…¨æ™¯", "æœ€æ–°iPhoneæ‰‹æ©Ÿè¦æ ¼æ¯”è¼ƒå’Œåƒ¹æ ¼åˆ†æ", "å¦‚ä½•åœ¨å®¶ç¨®æ¤è”¬èœçš„å®Œæ•´æŒ‡å—"]}
{"query": "æ©Ÿå™¨å­¸ç¿’å…¥é–€æ•™ç¨‹", "pos": ["Pythonæ©Ÿå™¨å­¸ç¿’åŸºç¤ï¼šå¾ç·šæ€§å›æ­¸åˆ°æ·±åº¦å­¸ç¿’çš„å®Œæ•´å­¸ç¿’è·¯å¾‘ï¼ŒåŒ…å«å¯¦ä½œç¯„ä¾‹å’Œä»£ç¢¼"], "neg": ["å°åŒ—å¤œå¸‚ç¾é£Ÿæ¨è–¦æ¸…å–®", "2025å¹´è‚¡å¸‚æŠ•è³‡ç­–ç•¥åˆ†æ", "å¯µç‰©ç‹—çš„æ—¥å¸¸ç…§è­·æ³¨æ„äº‹é …"]}
```

æ›´å¤šç¯„ä¾‹è«‹åƒè€ƒ `datasets/sample_data.jsonl`

## è¨“ç·´æŒ‡ä»¤

### 1. LoRA è¨“ç·´

é©åˆæœ‰é™GPUè¨˜æ†¶é«”çš„å ´æ™¯ï¼Œè¨“ç·´æ•ˆç‡é«˜ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python train.py \
  --model_name_or_path "/path/to/base/model" \
  --finetune_type lora \
  --train_dataset "/path/to/train_data.jsonl" \
  --output_dir "/path/to/output/lora_training" \
  --batch_size 1 \
  --lr 1e-4 \
  --epochs 3 \
  --neg_nums 4 \
  --temperature 0.02 \
  --query_max_len 128 \
  --passage_max_len 1000 \
  --save_on_epoch_end 1 \
  --warmup_proportion 0.05
```

### 2. QLoRA è¨“ç·´ (4-bité‡åŒ–)

é€²ä¸€æ­¥ç¯€çœè¨˜æ†¶é«”ï¼Œé©åˆå¤§æ¨¡å‹å¾®èª¿ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python train.py \
  --model_name_or_path "/path/to/base/model" \
  --finetune_type qlora \
  --quantization_bits 4 \
  --train_dataset "/path/to/train_data.jsonl" \
  --output_dir "/path/to/output/qlora_training" \
  --batch_size 1 \
  --lr 1e-4 \
  --epochs 3 \
  --neg_nums 2 \
  --temperature 0.02 \
  --query_max_len 128 \
  --passage_max_len 1000 \
  --save_on_epoch_end 1 \
  --warmup_proportion 0.05
```

### 3. å…¨åƒæ•¸å¾®èª¿

é©åˆå……è¶³è³‡æºç’°å¢ƒï¼Œæ•ˆæœé€šå¸¸æœ€ä½³ï¼š

```bash
CUDA_VISIBLE_DEVICES=0,1 python train.py \
  --model_name_or_path "/path/to/base/model" \
  --finetune_type full \
  --train_dataset "/path/to/train_data.jsonl" \
  --output_dir "/path/to/output/full_training" \
  --batch_size 1 \
  --lr 1e-5 \
  --epochs 3 \
  --neg_nums 1 \
  --temperature 0.02 \
  --query_max_len 128 \
  --passage_max_len 1000 \
  --save_on_epoch_end 1 \
  --warmup_proportion 0.05
```

## åƒæ•¸èªªæ˜

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `model_name_or_path` | åŸºç¤æ¨¡å‹è·¯å¾‘ | å¿…å¡« |
| `finetune_type` | å¾®èª¿é¡å‹: lora/qlora/full | qlora |
| `quantization_bits` | QLoRAé‡åŒ–ä½æ•¸: 4/8 | 4 |
| `train_dataset` | è¨“ç·´è³‡æ–™è·¯å¾‘ | å¿…å¡« |
| `output_dir` | è¼¸å‡ºç›®éŒ„ | å¿…å¡« |
| `batch_size` | æ‰¹æ¬¡å¤§å° | å¿…å¡« |
| `lr` | å­¸ç¿’ç‡ | 1e-4 |
| `epochs` | è¨“ç·´è¼ªæ•¸ | 2 |
| `neg_nums` | è² ä¾‹æ•¸é‡ | 2 |
| `temperature` | æº«åº¦åƒæ•¸ | 0.02 |
| `query_max_len` | æŸ¥è©¢æœ€å¤§é•·åº¦ | 256 |
| `passage_max_len` | æ–‡æª”æœ€å¤§é•·åº¦ | 1024 |
| `lora_r` | LoRA rank | 8 |
| `lora_alpha` | LoRA alpha | 32 |
| `lora_dropout` | LoRA dropout | 0.1 |

## æ¨¡å‹åˆä½µ

### LoRA/QLoRA æ¨¡å‹åˆä½µ

è¨“ç·´å®Œæˆå¾Œï¼Œéœ€è¦å°‡LoRA adapterèˆ‡base modelåˆä½µï¼š

```bash
python merge.py \
    --base_model_path "/path/to/base/model" \
    --lora_adapter_path "/path/to/lora/adapter.safetensors" \
    --output_path "/path/to/merged/model" \
    --training_precision "float32"
```

### åˆä½µåƒæ•¸èªªæ˜

- `base_model_path`: åŸå§‹åŸºç¤æ¨¡å‹è·¯å¾‘
- `lora_adapter_path`: LoRA adapterçš„safetensorsæª”æ¡ˆè·¯å¾‘
- `output_path`: åˆä½µå¾Œæ¨¡å‹çš„è¼¸å‡ºè·¯å¾‘
- `training_precision`: è¨“ç·´æ™‚ä½¿ç”¨çš„ç²¾åº¦ (float32/bfloat16)

## è¨“ç·´ç›£æ§

### TensorBoard ç›£æ§

```bash
tensorboard --logdir=/path/to/output/runs
```

### è¨“ç·´æ—¥èªŒ

è¨“ç·´éç¨‹æœƒç”Ÿæˆä»¥ä¸‹æ—¥èªŒæª”æ¡ˆï¼š

- `detailed_training_logs.json`: è©³ç´°çš„step-by-stepè¨“ç·´æ—¥èªŒ
- `training_losses.json`: æ¯å€‹epochçš„å¹³å‡loss
- `training_summary_report.json`: è¨“ç·´ç¸½çµå ±å‘Š

## è¼¸å‡ºçµæ§‹

è¨“ç·´å®Œæˆå¾Œçš„è¼¸å‡ºç›®éŒ„çµæ§‹ï¼š

```
output_dir/
â”œâ”€â”€ final/                        # æœ€çµ‚æ¨¡å‹
â”‚   â”œâ”€â”€ adapter_model.safetensors # LoRA adapter (åƒ…LoRA/QLoRA)
â”‚   â”œâ”€â”€ adapter_config.json       # LoRAé…ç½® (åƒ…LoRA/QLoRA)  
â”‚   â”œâ”€â”€ training_config.json      # è¨“ç·´é…ç½®
â”‚   â””â”€â”€ tokenizer files...        # Tokenizeræª”æ¡ˆ
â”œâ”€â”€ checkpoint-epoch-{N}/         # å„epochæª¢æŸ¥é»
â”œâ”€â”€ detailed_training_logs.json   # è©³ç´°è¨“ç·´æ—¥èªŒ
â”œâ”€â”€ training_losses.json          # Lossè¨˜éŒ„
â”œâ”€â”€ training_summary_report.json  # è¨“ç·´ç¸½çµ
â””â”€â”€ runs/                         # TensorBoardæ—¥èªŒ
```

## ä½¿ç”¨ç¯„ä¾‹

### 1. æº–å‚™è¨“ç·´è³‡æ–™

```python
# å»ºç«‹è¨“ç·´è³‡æ–™
import json

data = [
    {
        "query": "ä½ çš„æŸ¥è©¢",
        "pos": ["ç›¸é—œæ–‡æª”1", "ç›¸é—œæ–‡æª”2"],
        "neg": ["ä¸ç›¸é—œæ–‡æª”1", "ä¸ç›¸é—œæ–‡æª”2", "ä¸ç›¸é—œæ–‡æª”3"]
    }
]

with open("train_data.jsonl", "w", encoding="utf-8") as f:
    for item in data:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")
```

### 2. åŸ·è¡Œè¨“ç·´

```bash
# LoRAè¨“ç·´
python train.py \
  --model_name_or_path "Alibaba-NLP/gte-Qwen2-1.5B-instruct" \
  --finetune_type lora \
  --train_dataset "train_data.jsonl" \
  --output_dir "output/lora_training" \
  --batch_size 2 \
  --lr 1e-4 \
  --epochs 3
```

### 3. åˆä½µæ¨¡å‹

```bash
# åˆä½µLoRA adapter
python merge.py \
    --base_model_path "Alibaba-NLP/gte-Qwen2-1.5B-instruct" \
    --lora_adapter_path "output/lora_training/final/adapter_model.safetensors" \
    --output_path "output/merged_model"
```

### 4. ä½¿ç”¨æ¨¡å‹è©•ä¼°

> **æ³¨æ„**: å®Œæ•´çš„ä½¿ç”¨ç¯„ä¾‹è«‹åƒè€ƒ `evaluation.ipynb` æ–‡ä»¶

```python
import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel

# å®šç¾©last token poolingå‡½æ•¸ï¼Œæå–æ¯å€‹åºåˆ—ä¸­æœ€å¾Œä¸€å€‹æœ‰æ•ˆtokençš„embedding
def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

# è¼‰å…¥æ¨¡å‹å’Œtokenizer
model_path = "your/model/path"  # æ›¿æ›ç‚ºä½ çš„æ¨¡å‹è·¯å¾‘
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_path,
    torch_dtype=torch.float32,
    trust_remote_code=True,
    device_map="auto"  # è‡ªå‹•åˆ†é…GPU
)

# å®šç¾©æŸ¥è©¢å’Œæ–‡æª”
queries = ["ä½ çš„æŸ¥è©¢æ–‡æœ¬"]
documents = ["æ–‡æª”1", "æ–‡æª”2", "æ–‡æª”3"]

# åˆä½µæŸ¥è©¢å’Œæ–‡æª”é€²è¡Œæ‰¹æ¬¡è™•ç†
input_texts = queries + documents
batch_dict = tokenizer(
    input_texts, 
    max_length=512, 
    padding=True, 
    truncation=True, 
    return_tensors='pt'
)

# ç§»å‹•åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
if torch.cuda.is_available():
    batch_dict = {k: v.cuda() for k, v in batch_dict.items()}

# ç²å–æ¨¡å‹è¼¸å‡ºä¸¦æå–embeddings
with torch.no_grad():
    outputs = model(**batch_dict, output_hidden_states=True)
    embeddings = last_token_pool(outputs.hidden_states[-1], batch_dict['attention_mask'])
    embeddings = F.normalize(embeddings, p=2, dim=1)

# è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
scores = (embeddings[:len(queries)] @ embeddings[len(queries):].T) * 100

# é¡¯ç¤ºçµæœ
for i, query in enumerate(queries):
    print(f"æŸ¥è©¢: {query}")
    query_scores = scores[i].tolist()
    for j, (doc, score) in enumerate(zip(documents, query_scores)):
        print(f"  æ–‡æª”{j+1}: {score:.2f} - {doc}")
```

## æ³¨æ„äº‹é …

1. **è¨˜æ†¶é«”ç®¡ç†**: QLoRAéœ€è¦è¼ƒå°‘GPUè¨˜æ†¶é«”ï¼Œé©åˆè³‡æºå—é™ç’°å¢ƒ
2. **å­¸ç¿’ç‡èª¿æ•´**: ä¸åŒå¾®èª¿æ–¹å¼å»ºè­°ä½¿ç”¨ä¸åŒå­¸ç¿’ç‡ (Full: 1e-5, LoRA/QLoRA: 1e-4)
3. **è² ä¾‹æ•¸é‡**: å»ºè­°æ ¹æ“šGPUè¨˜æ†¶é«”èª¿æ•´neg_numsåƒæ•¸
4. **ç²¾åº¦ä¸€è‡´æ€§**: åˆä½µæ™‚ç¢ºä¿training_precisionèˆ‡è¨“ç·´æ™‚ä¸€è‡´

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **CUDA OOM**: æ¸›å°‘batch_sizeæˆ–ä½¿ç”¨QLoRA
2. **åˆä½µå¤±æ•—**: æª¢æŸ¥LoRAé…ç½®æ˜¯å¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´
3. **ç²¾åº¦ä¸åŒ¹é…**: ç¢ºä¿è¨“ç·´å’Œåˆä½µä½¿ç”¨ç›¸åŒçš„ç²¾åº¦è¨­å®š

### æ•ˆèƒ½å„ªåŒ–

- ä½¿ç”¨å¤šGPU: åœ¨CUDA_VISIBLE_DEVICESä¸­æŒ‡å®šå¤šå€‹GPU
- èª¿æ•´num_workers: æ ¹æ“šCPUæ ¸å¿ƒæ•¸èª¿æ•´DataLoaderçš„num_workers
- ä½¿ç”¨æ¢¯åº¦ç´¯ç©: å¢åŠ gradient_accumulation_stepsä»¥æ¨¡æ“¬æ›´å¤§çš„batch size

## License

MIT License 