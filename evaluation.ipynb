{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "# Optional: Set visible GPUs (optional, if you want to limit usage to specific GPUs)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "\n",
    "# Define last token pooling function to extract the embedding of the last valid token in each sequence\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    # Check if left padding is used\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        # Compute the actual length of each sequence to locate the last valid token\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model_path = \"your/merged/model/path\" \n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load model with device_map=\"auto\" to distribute across multiple GPUs\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Define query\n",
    "queries = [\"your query text\"]\n",
    "\n",
    "# Define document database\n",
    "documents = [\"document 1\", \"document 2\", \"document 3\"]\n",
    "\n",
    "# Merge queries and documents into a single batch for processing\n",
    "input_texts = queries + documents\n",
    "max_length = 850\n",
    "\n",
    "print(f\"Processing {len(queries)} queries and {len(documents)} documents...\")\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(\n",
    "    input_texts, \n",
    "    max_length=max_length, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Move input data to CUDA (automatically selects appropriate device)\n",
    "if torch.cuda.is_available():\n",
    "    batch_dict = {k: v.cuda() for k, v in batch_dict.items()}\n",
    "else:\n",
    "    print(\"Warning: CUDA not detected. Using CPU for inference.\")\n",
    "\n",
    "print(\"Running inference...\")\n",
    "\n",
    "# Get hidden states from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch_dict, output_hidden_states=True)\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "# Extract embeddings using last token pooling\n",
    "embeddings = last_token_pool(last_hidden_states, batch_dict['attention_mask'])\n",
    "\n",
    "# Normalize embeddings with L2 norm\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity between query and documents and scale scores\n",
    "scores = (embeddings[:len(queries)] @ embeddings[len(queries):].T) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Similarity Analysis Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for query_idx, query in enumerate(queries):\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"\\nSimilarity scores for each document:\")\n",
    "    \n",
    "    query_scores = scores[query_idx].tolist()\n",
    "    \n",
    "    # Pair documents with their scores and sort by score descending\n",
    "    doc_score_pairs = list(zip(documents, query_scores, range(len(documents))))\n",
    "    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (doc, score, original_idx) in enumerate(doc_score_pairs):\n",
    "        doc_preview = doc[:50] + \"...\" if len(doc) > 50 else doc\n",
    "        print(f\"  Rank {i+1} - Document {original_idx+1}: {score:.2f} - {doc_preview}\")\n",
    "    \n",
    "    print(f\"\\nRaw scores: {[f'{s:.2f}' for s in query_scores]}\")\n",
    "    best_doc_idx = query_scores.index(max(query_scores))\n",
    "    print(f\"Highest similarity: Document {best_doc_idx+1} (Score: {max(query_scores):.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gte2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
